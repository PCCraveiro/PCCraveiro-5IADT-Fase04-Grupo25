{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "authorship_tag": "ABX9TyN4fxWzLKf3t/2UtYuUDzXL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3e07a54a3dc544d8b3a7e2d3158d51df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8a57f424a163424296334894ab33eaca",
              "IPY_MODEL_f3eae2f52349491e89573aacedcb5dd0",
              "IPY_MODEL_f83f88babfc7420bada82c43932ccf0b"
            ],
            "layout": "IPY_MODEL_705e6c1237974dddad82346df330c5ef"
          }
        },
        "8a57f424a163424296334894ab33eaca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3eb59027f0d0440db492654e206f095b",
            "placeholder": "​",
            "style": "IPY_MODEL_ec43ff50eb2242b3ba8d411ad317736a",
            "value": "model.pt: 100%"
          }
        },
        "f3eae2f52349491e89573aacedcb5dd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be1084ebf92f479789fffc01050276da",
            "max": 6247065,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_41f09853066c4b85857255404cf5666c",
            "value": 6247065
          }
        },
        "f83f88babfc7420bada82c43932ccf0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c90a33bd7f644b3983b2346f33f4361c",
            "placeholder": "​",
            "style": "IPY_MODEL_d1b1ac3213a44d62861fd68c266ad4aa",
            "value": " 6.25M/6.25M [00:01&lt;00:00, 4.40MB/s]"
          }
        },
        "705e6c1237974dddad82346df330c5ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3eb59027f0d0440db492654e206f095b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec43ff50eb2242b3ba8d411ad317736a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "be1084ebf92f479789fffc01050276da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41f09853066c4b85857255404cf5666c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c90a33bd7f644b3983b2346f33f4361c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1b1ac3213a44d62861fd68c266ad4aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PCCraveiro/PCCraveiro-5IADT-Fase04-Grupo25/blob/main/Tech_Challenge_IADT_Fase_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGr-N3o1NblT",
        "outputId": "ba5bd700-9fbe-44cf-d476-6690b22aafe7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "✅ Drive montado\n"
          ]
        }
      ],
      "source": [
        "# Monta drive (ggogle drive)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print(\"✅ Drive montado\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalações / Atualizações\n",
        "\n",
        "!pip install -q --upgrade pip\n",
        "\n",
        "# Core do projeto\n",
        "!pip install -q ultralytics fer opencv-python tqdm\n",
        "!pip install -q --upgrade ultralytics\n",
        "!pip install -q fer tqdm\n",
        "\n",
        "print(\"✅ Instalações / Atualizações\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zX51fIpRWoR1",
        "outputId": "0a922844-6fd4-46d8-8c54-a6fdd66cda40"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h✅ Instalações / Atualizações\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from ultralytics import YOLO\n",
        "from fer.fer import FER\n",
        "from collections import Counter, defaultdict, deque\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "print(\"✅ Bibliotecas importadas\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2MR0rrHclD4",
        "outputId": "1074fa41-e67f-4e53-c9cd-51a94716df05"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "✅ Bibliotecas importadas\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PARÂMETROS\n",
        "\n",
        "# Pasta base do projeto dentro do seu Drive\n",
        "DRIVE_BASE_DIR = \"/content/drive/MyDrive/Colab Notebooks/Fase_4/\"\n",
        "\n",
        "os.makedirs(DRIVE_BASE_DIR, exist_ok=True)\n",
        "\n",
        "# Paths de entrada/saída no Drive\n",
        "VIDEO_PATH = os.path.join(DRIVE_BASE_DIR, \"video.mp4\")\n",
        "OUTPUT_VIDEO = os.path.join(DRIVE_BASE_DIR, \"video_analisado_v2.mp4\")\n",
        "REPORT_PATH = os.path.join(DRIVE_BASE_DIR, \"relatorio_v2.json\")\n",
        "\n",
        "POSE_MODEL_PATH = \"yolo11m-pose.pt\"     # +13% precisão, ainda rápido\n",
        "\n",
        "CONF_THRESHOLD = 0.35  # mais sensível\n",
        "EMOTION_CONF_THRESHOLD = 0.4  # confiança mínima para aceitar emoção\n",
        "IOU_THRESHOLD = 0.7\n",
        "DEQUE_MAX_LEN = 12\n",
        "\n",
        "pose_history = defaultdict(lambda: deque(maxlen=5))\n",
        "speed_history = defaultdict(lambda: deque(maxlen=10))\n",
        "action_history = defaultdict(lambda: deque(maxlen=5))\n",
        "\n",
        "ANOMALY_SPEED_THRESHOLD = 35.0   # ajuste fino\n",
        "ANOMALY_ZSCORE = 2.5\n",
        "\n",
        "# --- ANOMALIAS ---\n",
        "ANOMALY_SPEED_Z = 2.5\n",
        "ANOMALY_MIN_SPEED = 20.0\n",
        "ANOMALY_ACTION_WINDOW = 5\n",
        "anomaly_count = 0\n",
        "\n",
        "# Cores em BGR\n",
        "COLORS = {\n",
        "    \"happy\": (0, 255, 255),\n",
        "    \"sad\": (255, 0, 0),\n",
        "    \"angry\": (0, 0, 255),\n",
        "    \"surprise\": (255, 255, 0),\n",
        "    \"fear\": (255, 0, 255),\n",
        "    \"neutral\": (200, 200, 200),\n",
        "    \"disgust\": (0, 255, 0)\n",
        "}\n",
        "\n",
        "FACE_MODEL_PATH = hf_hub_download(\n",
        "    repo_id=\"arnabdhar/YOLOv8-Face-Detection\",\n",
        "    filename=\"model.pt\",\n",
        "    local_dir=\"/content/models\"\n",
        ")\n",
        "\n",
        "# Baixa tracker otimizado\n",
        "!curl -O https://github.com/ultralytics/ultralytics/raw/main/ultralytics/cfg/trackers/bytetrack.yaml\n",
        "\n",
        "print(f\"✅ Face model baixado: {FACE_MODEL_PATH}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "3e07a54a3dc544d8b3a7e2d3158d51df",
            "8a57f424a163424296334894ab33eaca",
            "f3eae2f52349491e89573aacedcb5dd0",
            "f83f88babfc7420bada82c43932ccf0b",
            "705e6c1237974dddad82346df330c5ef",
            "3eb59027f0d0440db492654e206f095b",
            "ec43ff50eb2242b3ba8d411ad317736a",
            "be1084ebf92f479789fffc01050276da",
            "41f09853066c4b85857255404cf5666c",
            "c90a33bd7f644b3983b2346f33f4361c",
            "d1b1ac3213a44d62861fd68c266ad4aa"
          ]
        },
        "id": "HvLoFhT4WrzO",
        "outputId": "e157a36a-9db8-432d-de05-29e6d9bfd274"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.pt:   0%|          | 0.00/6.25M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3e07a54a3dc544d8b3a7e2d3158d51df"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "✅ Face model baixado: /content/models/model.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DETECTOR DE CENA\n",
        "class SceneDetector:\n",
        "    \"\"\"\n",
        "    Detector simples de mudança de cena usando histograma de cor em HSV.\n",
        "    Quando a correlação entre histograma atual e anterior fica abaixo\n",
        "    do limiar, considera-se que há uma nova cena.\n",
        "    \"\"\"\n",
        "    def __init__(self, threshold=0.7):\n",
        "        self.threshold = threshold\n",
        "        self.last_hist = None\n",
        "\n",
        "    def detect_scene_change(self, frame):\n",
        "        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
        "        hist = cv2.calcHist([hsv], [0, 1], None, [50, 60], [0, 180, 0, 256])\n",
        "        cv2.normalize(hist, hist, 0, 1, cv2.NORM_MINMAX)\n",
        "\n",
        "        is_new_scene = False\n",
        "        if self.last_hist is not None:\n",
        "            score = cv2.compareHist(self.last_hist, hist, cv2.HISTCMP_CORREL)\n",
        "            if score < self.threshold:\n",
        "                is_new_scene = True\n",
        "\n",
        "        self.last_hist = hist\n",
        "        return is_new_scene"
      ],
      "metadata": {
        "id": "A59T_RlNqZ4H"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CLASSIFICADOR DE AÇÃO\n",
        "class ActionClassifier:\n",
        "    def __init__(self):\n",
        "        self.history = {}\n",
        "        self.velocities = {}\n",
        "\n",
        "    def update(self, track_id, keypoints):\n",
        "        if track_id not in self.history:\n",
        "            self.history[track_id] = deque(maxlen=30)\n",
        "            self.velocities[track_id] = deque(maxlen=10)\n",
        "\n",
        "        self.history[track_id].append(keypoints)\n",
        "\n",
        "        if len(self.history[track_id]) >= 2:\n",
        "            prev_hip = self.history[track_id][-2][11]\n",
        "            curr_hip = keypoints[11]\n",
        "            if prev_hip[0] > 0 and curr_hip[0] > 0:\n",
        "                speed = np.linalg.norm(curr_hip - prev_hip)\n",
        "                self.velocities[track_id].append(speed)\n",
        "\n",
        "    def classify(self, keypoints, track_id):\n",
        "        kpts = keypoints\n",
        "        if kpts.shape[0] < 17:\n",
        "            return \"Desconhecido\"\n",
        "\n",
        "        # 1. QUEDA\n",
        "        shoulder_y = np.mean([kpts[5][1], kpts[6][1]])\n",
        "        hip_y = np.mean([kpts[11][1], kpts[12][1]])\n",
        "        ankle_y = np.mean([kpts[15][1], kpts[16][1]])\n",
        "\n",
        "        if (shoulder_y > 0 and hip_y > 0 and ankle_y > 0 and\n",
        "            shoulder_y > hip_y * 0.9 and hip_y > ankle_y * 0.8):\n",
        "            return \"DEITADO\"\n",
        "\n",
        "        # 2. MÃOS PARA CIMA\n",
        "        nose_y = kpts[0][1]\n",
        "        wrists_y = [kpts[9][1], kpts[10][1]]\n",
        "        if nose_y > 0 and any(w > 0 and w < nose_y * 0.8 for w in wrists_y):\n",
        "            return \"MAOS PARA CIMA\"\n",
        "\n",
        "        # 3. SENTADO (anti-oclução + mesa)\n",
        "        valid_kpts = kpts[kpts[:,0] > 0]\n",
        "        if len(valid_kpts) > 8:\n",
        "            # Critério 1: pernas dobradas OU tronco baixo\n",
        "            knee_hip_dist = (np.linalg.norm(kpts[11] - kpts[13]) +\n",
        "                            np.linalg.norm(kpts[12] - kpts[14]))\n",
        "            shoulder_hip_dist = np.linalg.norm(kpts[5] - kpts[11]) + np.linalg.norm(kpts[6] - kpts[12])\n",
        "\n",
        "            # Critério 2: braços na mesa (cotovelos próximos)\n",
        "            elbow_dist = np.linalg.norm(kpts[7] - kpts[8])\n",
        "\n",
        "            if knee_hip_dist < shoulder_hip_dist * 0.8 or elbow_dist < 80:\n",
        "                return \"SENTADO\"\n",
        "\n",
        "        # 4. CAMINHANDO/CORRENDO (CORRIGIDO)\n",
        "        avg_speed = 0\n",
        "        if track_id in self.velocities and len(self.velocities[track_id]) >= 5:\n",
        "            avg_speed = np.mean(self.velocities[track_id])\n",
        "\n",
        "            if avg_speed > 15:\n",
        "                return \"CORRENDO\"\n",
        "            elif avg_speed > 8:\n",
        "                return \"CAMINHANDO\"\n",
        "\n",
        "        return \"EM PE\""
      ],
      "metadata": {
        "id": "fP1RsX0eqk8X"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DASHBOARD OVERLAY\n",
        "def draw_dashboard(img, frame_emotions, scene_num, scene_people_count, total_unique_people):\n",
        "    \"\"\"\n",
        "    Desenha um pequeno dashboard sobre o frame com:\n",
        "    - Número da cena\n",
        "    - Pessoas na cena e total global\n",
        "    - Barras de emoções detectadas no frame atual\n",
        "    \"\"\"\n",
        "    overlay = img.copy()\n",
        "    h, w, _ = img.shape\n",
        "    dash_w = 280\n",
        "    dash_h = 240\n",
        "    x_start = w - dash_w - 20\n",
        "    y_start = 20\n",
        "\n",
        "    cv2.rectangle(overlay, (x_start, y_start), (x_start + dash_w, y_start + dash_h), (0, 0, 0), -1)\n",
        "    alpha = 0.6\n",
        "    img = cv2.addWeighted(overlay, alpha, img, 1 - alpha, 0)\n",
        "\n",
        "    cv2.putText(img, \"DASHBOARD\", (x_start + 80, y_start + 25),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
        "\n",
        "    y_off = y_start + 55\n",
        "    cv2.putText(img, f\"Cena: {scene_num}\", (x_start + 10, y_off),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
        "    y_off += 25\n",
        "    cv2.putText(img, f\"Pessoas (Cena): {scene_people_count}\", (x_start + 10, y_off),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
        "    y_off += 25\n",
        "    cv2.putText(img, f\"Pessoas (Total): {total_unique_people}\", (x_start + 10, y_off),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (200, 200, 200), 1)\n",
        "\n",
        "    y_off += 30\n",
        "    cv2.putText(img, \"EMOCOES (Ao Vivo)\", (x_start + 10, y_off),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
        "    y_off += 25\n",
        "\n",
        "    total = sum(frame_emotions.values()) if frame_emotions else 1\n",
        "    order = [\"happy\", \"neutral\", \"surprise\", \"fear\", \"angry\", \"sad\"]\n",
        "    for emo in order:\n",
        "        count = frame_emotions.get(emo, 0)\n",
        "        color = COLORS.get(emo, (255, 255, 255))\n",
        "        cv2.putText(img, f\"{emo.capitalize()}\", (x_start + 10, y_off),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
        "        bar_w_max = 120\n",
        "        bar_w = int((count / total) * bar_w_max) if total > 0 else 0\n",
        "        if count > 0 and bar_w < 5:\n",
        "            bar_w = 5\n",
        "        cv2.rectangle(img, (x_start + 90, y_off - 10),\n",
        "                      (x_start + 90 + bar_w, y_off), color, -1)\n",
        "        cv2.putText(img, f\"{count}\", (x_start + 220, y_off),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
        "        y_off += 20\n",
        "\n",
        "    return img"
      ],
      "metadata": {
        "id": "CDlkB2ZfqxUD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ASSOCIAÇÃO ROSTO-PESSOA\n",
        "def get_face_for_person(kpts, face_boxes):\n",
        "    \"\"\"\n",
        "    Encontra a melhor bounding box de rosto para um esqueleto (pessoa).\n",
        "    Usa a média dos keypoints da cabeça (nariz, olhos, orelhas) como centro\n",
        "    e escolhe o rosto mais próximo.\n",
        "    \"\"\"\n",
        "    head_pts = kpts[0:5]\n",
        "    valid_head = head_pts[head_pts[:, 0] > 0]\n",
        "\n",
        "    if len(valid_head) == 0:\n",
        "        return None\n",
        "\n",
        "    head_center = np.mean(valid_head, axis=0)\n",
        "\n",
        "    best_match = None\n",
        "    min_dist = float('inf')\n",
        "\n",
        "    for box in face_boxes:\n",
        "        x1, y1, x2, y2 = box[:4]\n",
        "        face_center = np.array([(x1 + x2) / 2, (y1 + y2) / 2])\n",
        "        dist = np.linalg.norm(head_center - face_center)\n",
        "\n",
        "        if dist < 100:  # limiar em pixels (ajuste conforme resolução)\n",
        "            if dist < min_dist:\n",
        "                min_dist = dist\n",
        "                best_match = box\n",
        "\n",
        "    return best_match"
      ],
      "metadata": {
        "id": "cok_750vq2BC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_anomaly(track_id, action, action_classifier, action_history):\n",
        "    is_speed_anomaly = False\n",
        "    is_action_anomaly = False\n",
        "\n",
        "    # --- 1) VELOCIDADE ANORMAL ---\n",
        "    speeds = action_classifier.velocities.get(track_id, [])\n",
        "\n",
        "    if len(speeds) >= 5:\n",
        "        speeds_arr = np.array(speeds)\n",
        "        mean = speeds_arr.mean()\n",
        "        std = speeds_arr.std() + 1e-6\n",
        "        z = (speeds_arr[-1] - mean) / std\n",
        "\n",
        "        if speeds_arr[-1] > ANOMALY_MIN_SPEED or z > ANOMALY_SPEED_Z:\n",
        "            is_speed_anomaly = True\n",
        "\n",
        "    # --- 2) MUDANÇA BRUSCA DE AÇÃO ---\n",
        "    action_history[track_id].append(action)\n",
        "\n",
        "    if len(action_history[track_id]) >= 3:\n",
        "        last_actions = list(action_history[track_id])\n",
        "        # ação atual apareceu só uma vez na janela\n",
        "        if last_actions.count(last_actions[-1]) == 1:\n",
        "            is_action_anomaly = True\n",
        "\n",
        "    return is_speed_anomaly or is_action_anomaly\n"
      ],
      "metadata": {
        "id": "KArxIU6wHH0b"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PROCESSAMENTO DO VÍDEO\n",
        "def process_video():\n",
        "    \"\"\"\n",
        "    Pipeline principal:\n",
        "    - Carrega modelos de pose e rosto (YOLO) em GPU se disponível\n",
        "    - Percorre o vídeo frame a frame\n",
        "    - Detecta pessoas, ações, rostos e emoções\n",
        "    - Detecta mudança de cena\n",
        "    - Gera vídeo anotado e relatório JSON\n",
        "    \"\"\"\n",
        "    # Dispositivo: prioriza CUDA (A100), caso não exista cai para CPU\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    print(\"Loading models...\")\n",
        "    # Ultralytics carrega automaticamente o modelo local ou baixa da internet\n",
        "    pose_model = YOLO(POSE_MODEL_PATH)  # modelo de pose\n",
        "    try:\n",
        "        face_model = YOLO(FACE_MODEL_PATH)  # modelo de face\n",
        "        print(\"Face model loaded successfully.\")\n",
        "    except Exception as e:\n",
        "        face_model = None\n",
        "        print(f\"WARNING: Face model failed to load: {e}\")\n",
        "\n",
        "    # Detector de emoções FER (usa internamente CNN + OpenCV)\n",
        "    detector = FER(mtcnn=False)\n",
        "\n",
        "    # Abre o vídeo (no Drive)\n",
        "    cap = cv2.VideoCapture(VIDEO_PATH)\n",
        "    if not cap.isOpened():\n",
        "        raise FileNotFoundError(f\"Não foi possível abrir o vídeo em: {VIDEO_PATH}\")\n",
        "\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS) or 25.0\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    # Writer para o vídeo anotado (no Drive)\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    writer = cv2.VideoWriter(OUTPUT_VIDEO, fourcc, fps, (width, height))\n",
        "\n",
        "    action_classifier = ActionClassifier()\n",
        "    scene_detector = SceneDetector(threshold=0.7)\n",
        "\n",
        "    # --- HISTÓRICO DE AÇÕES\n",
        "    action_history = defaultdict(lambda: deque(maxlen=5))\n",
        "\n",
        "    # Estatísticas finais (para o JSON)\n",
        "    stats = {\n",
        "        \"emocoes\": Counter(),\n",
        "        \"acoes\": Counter(),\n",
        "        \"frames\": 0,\n",
        "        \"total_cenas\": 0,\n",
        "        \"pessoas_por_cena\": {},  # {scene_id: count}\n",
        "        \"pessoas_global\": set(),  # será convertido para lista na hora do dump\n",
        "        \"anomalias\": 0,\n",
        "        \"anomalias_por_frame\": []\n",
        "    }\n",
        "\n",
        "    emotion_buffer = {}\n",
        "    last_emotions = {}\n",
        "\n",
        "    # Controle de cenas e IDs locais\n",
        "    scene_counter = 1\n",
        "    scene_map = {}  # {global_track_id: local_scene_id}\n",
        "    next_scene_person_id = 1\n",
        "    current_scene_people = set()\n",
        "\n",
        "    pbar = tqdm(total=total_frames, desc=\"Processing V2 Colab\", ncols=100)\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        stats[\"frames\"] += 1\n",
        "        annotated_frame = frame.copy()\n",
        "        current_frame_emotions = Counter()\n",
        "\n",
        "        # 1) Detecção de mudança de cena\n",
        "        if scene_detector.detect_scene_change(frame):\n",
        "            scene_counter += 1\n",
        "            scene_map = {}\n",
        "            next_scene_person_id = 1\n",
        "            current_scene_people = set()\n",
        "            action_classifier.history = {}\n",
        "            print(f\"Frame {stats['frames']}: Nova cena detectada ({scene_counter})\")\n",
        "\n",
        "        # 2) Detecção de rostos (opcional)\n",
        "        face_boxes = []\n",
        "        if face_model:\n",
        "            face_results = face_model(frame, verbose=False, conf=0.4, device=device, half=True)\n",
        "            if face_results and face_results[0].boxes is not None:\n",
        "                face_boxes = face_results[0].boxes.xyxy.detach().cpu().numpy()\n",
        "\n",
        "        # 3) Pose + tracking\n",
        "        pose_results = pose_model.track(\n",
        "            frame,\n",
        "            persist=True,\n",
        "            verbose=False,\n",
        "            device=device,\n",
        "            conf=CONF_THRESHOLD,\n",
        "            iou=IOU_THRESHOLD,\n",
        "            classes=[0], # só pessoas\n",
        "            half=True  # FP16\n",
        "        )\n",
        "\n",
        "        # FILTRA POSES RUINS (robôs, objetos)\n",
        "        if pose_results and pose_results[0].keypoints is not None:\n",
        "\n",
        "            kpts_all = pose_results[0].keypoints.xy.detach().cpu().numpy()\n",
        "\n",
        "            if pose_results[0].keypoints.conf is not None:\n",
        "                kpts_conf = pose_results[0].keypoints.conf.detach().cpu().numpy()\n",
        "            else:\n",
        "                kpts_conf = np.ones((kpts_all.shape[0], kpts_all.shape[1]))\n",
        "\n",
        "            ids = pose_results[0].boxes.id\n",
        "            if ids is not None:\n",
        "                ids = ids.int().detach().cpu().tolist()\n",
        "            else:\n",
        "                ids = [-1] * len(kpts_all)\n",
        "\n",
        "            valid_indices = []\n",
        "\n",
        "            for i, (kpts, confs) in enumerate(zip(kpts_all, kpts_conf)):\n",
        "                num_good_kpts = np.sum(\n",
        "                    (kpts[:, 0] > 0) &\n",
        "                    (kpts[:, 1] > 0) &\n",
        "                    (confs > 0.3)\n",
        "                )\n",
        "                if num_good_kpts >= 10:\n",
        "                    valid_indices.append(i)\n",
        "\n",
        "            # aplica filtro UMA ÚNICA VEZ\n",
        "            if valid_indices:\n",
        "                kpts_all = kpts_all[valid_indices]\n",
        "                ids = [ids[i] if i < len(ids) else -1 for i in valid_indices]\n",
        "            else:\n",
        "                kpts_all = []\n",
        "                ids = []\n",
        "\n",
        "        if pose_results and pose_results[0].keypoints is not None:\n",
        "            kpts_all = pose_results[0].keypoints.xy.detach().cpu().numpy()\n",
        "            ids = pose_results[0].boxes.id\n",
        "            if ids is not None:\n",
        "                ids = ids.int().detach().cpu().tolist()\n",
        "            else:\n",
        "                ids = [-1] * len(kpts_all)\n",
        "\n",
        "            for kpts, track_id in zip(kpts_all, ids):\n",
        "                # Mapeamento de ID global -> ID local na cena\n",
        "                if track_id != -1:\n",
        "                    stats[\"pessoas_global\"].add(int(track_id))\n",
        "                    if track_id not in scene_map:\n",
        "                        scene_map[track_id] = next_scene_person_id\n",
        "                        next_scene_person_id += 1\n",
        "\n",
        "                    local_id = scene_map[track_id]\n",
        "                    current_scene_people.add(local_id)\n",
        "                else:\n",
        "                    local_id = \"?\"\n",
        "\n",
        "                # Atualiza classificador de ação\n",
        "                action_classifier.update(track_id, kpts)\n",
        "                action = action_classifier.classify(kpts, track_id)\n",
        "                stats[\"acoes\"][action] += 1\n",
        "\n",
        "                # --- DETECÇÃO DE ANOMALIA ---\n",
        "                is_anomaly = False\n",
        "                if track_id != -1:\n",
        "                    is_anomaly = detect_anomaly(\n",
        "                        track_id,\n",
        "                        action,\n",
        "                        action_classifier,\n",
        "                        action_history\n",
        "                    )\n",
        "\n",
        "                    if is_anomaly:\n",
        "                        stats[\"anomalias\"] += 1\n",
        "                        stats[\"anomalias_por_frame\"].append({\n",
        "                            \"frame\": stats[\"frames\"],\n",
        "                            \"track_id\": int(track_id),\n",
        "                            \"action\": action,\n",
        "                            \"speed\": float(\n",
        "                                action_classifier.velocities[track_id][-1]\n",
        "                                if action_classifier.velocities.get(track_id)\n",
        "                                else 0\n",
        "                            )\n",
        "                        })\n",
        "\n",
        "                # Desenha esqueleto\n",
        "                skeleton = [\n",
        "                    (5, 7), (7, 9), (6, 8), (8, 10),\n",
        "                    (5, 6), (5, 11), (6, 12),\n",
        "                    (11, 12), (11, 13), (13, 15),\n",
        "                    (12, 14), (14, 16)\n",
        "                ]\n",
        "                for p1, p2 in skeleton:\n",
        "                    if kpts[p1][0] > 0 and kpts[p2][0] > 0:\n",
        "                        pt1 = (int(kpts[p1][0]), int(kpts[p1][1]))\n",
        "                        pt2 = (int(kpts[p2][0]), int(kpts[p2][1]))\n",
        "                        cv2.line(annotated_frame, pt1, pt2, (0, 255, 0), 2)\n",
        "\n",
        "                # Emoções\n",
        "                emotion_label = last_emotions.get(track_id, \"neutral\")\n",
        "\n",
        "                # Tenta associar um rosto detectado a essa pessoa\n",
        "                matched_face = get_face_for_person(kpts, face_boxes)\n",
        "                face_img = None\n",
        "\n",
        "                if matched_face is not None:\n",
        "                    x1, y1, x2, y2 = matched_face[:4].astype(int)\n",
        "                    pad = 10\n",
        "                    x1 = max(0, x1 - pad)\n",
        "                    y1 = max(0, y1 - pad)\n",
        "                    x2 = min(width, x2 + pad)\n",
        "                    y2 = min(height, y2 + pad)\n",
        "                    if x2 > x1 and y2 > y1:\n",
        "                        face_img = frame[y1:y2, x1:x2]\n",
        "\n",
        "                # fallback: recorta região da cabeça a partir dos keypoints\n",
        "                elif np.any(kpts[0:5, 0] > 0):\n",
        "                    face_pts = kpts[0:5]\n",
        "                    valid_face_pts = face_pts[face_pts[:, 0] > 0]\n",
        "                    if len(valid_face_pts) > 0:\n",
        "                        fx1, fy1 = np.min(valid_face_pts, axis=0).astype(int)\n",
        "                        fx2, fy2 = np.max(valid_face_pts, axis=0).astype(int)\n",
        "                        pad = 30\n",
        "                        fx1 = max(0, fx1 - pad)\n",
        "                        fy1 = max(0, fy1 - pad)\n",
        "                        fx2 = min(width, fx2 + pad)\n",
        "                        fy2 = min(height, fy2 + pad)\n",
        "                        if fx2 > fx1 and fy2 > fy1:\n",
        "                            face_img = frame[fy1:fy2, fx1:fx2]\n",
        "\n",
        "                # Inferência de emoção se tivermos uma face recortada válida\n",
        "                if face_img is not None and face_img.size > 0 and face_img.shape[0] > 20 and face_img.shape[1] > 20:\n",
        "                    try:\n",
        "                        emotions = detector.detect_emotions(face_img)\n",
        "                        if emotions:\n",
        "                            top_emotion, score = max(\n",
        "                                emotions[0][\"emotions\"].items(),\n",
        "                                key=lambda x: x[1]\n",
        "                            )\n",
        "                            if score > EMOTION_CONF_THRESHOLD:\n",
        "                                if track_id not in emotion_buffer:\n",
        "                                    emotion_buffer[track_id] = deque(maxlen=DEQUE_MAX_LEN)\n",
        "                                emotion_buffer[track_id].append(top_emotion)\n",
        "                                most_common = Counter(emotion_buffer[track_id]).most_common(1)[0][0]\n",
        "                                emotion_label = most_common\n",
        "                                last_emotions[track_id] = emotion_label\n",
        "                    except Exception:\n",
        "                        pass\n",
        "\n",
        "                stats[\"emocoes\"][emotion_label] += 1\n",
        "                current_frame_emotions[emotion_label] += 1\n",
        "\n",
        "                # Desenha label da pessoa\n",
        "                if np.any(kpts[:, 0] > 0):\n",
        "                    valid_x = kpts[kpts[:, 0] > 0, 0]\n",
        "                    valid_y = kpts[kpts[:, 0] > 0, 1]\n",
        "                    x1, y1 = int(np.min(valid_x)), int(np.min(valid_y))\n",
        "\n",
        "                    color = COLORS.get(emotion_label, (0, 255, 0))\n",
        "\n",
        "                    # Calcula proximidade com outras pessoas\n",
        "                    # Label melhorado com contexto grupo\n",
        "                    nearby_count = len(current_scene_people) - 1  # total na cena menos eu\n",
        "                    label_text = f\"Pessoa {local_id} | {action}\"\n",
        "                    if nearby_count > 1:\n",
        "                        label_text += f\" | Grupo:{nearby_count}\"\n",
        "\n",
        "                    emo_label = f\"{emotion_label.upper()}\"\n",
        "\n",
        "                    (tw, th), _ = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n",
        "                    cv2.rectangle(annotated_frame, (x1, y1-50), (x1 + max(220, tw), y1), color, -1)\n",
        "                    cv2.putText(annotated_frame, label_text, (x1, y1-25), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2)\n",
        "                    cv2.putText(annotated_frame, emo_label, (x1, y1-5), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
        "\n",
        "                    if is_anomaly:\n",
        "                        cv2.putText(\n",
        "                            annotated_frame,\n",
        "                            \"ANOMALIA\",\n",
        "                            (x1, y1 - 70),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                            0.7,\n",
        "                            (0, 0, 255),\n",
        "                            3\n",
        "                        )\n",
        "\n",
        "        # Atualiza estatísticas por cena\n",
        "        stats[\"total_cenas\"] = scene_counter\n",
        "        stats[\"pessoas_por_cena\"][f\"Cena {scene_counter}\"] = len(current_scene_people)\n",
        "\n",
        "        annotated_frame = draw_dashboard(\n",
        "            annotated_frame,\n",
        "            current_frame_emotions,\n",
        "            scene_counter,\n",
        "            len(current_scene_people),\n",
        "            len(stats[\"pessoas_global\"])\n",
        "        )\n",
        "\n",
        "        writer.write(annotated_frame)\n",
        "        pbar.update(1)\n",
        "\n",
        "    cap.release()\n",
        "    writer.release()\n",
        "    pbar.close()\n",
        "\n",
        "    # Converte set para lista para serializar em JSON\n",
        "    stats[\"pessoas_global\"] = list(stats[\"pessoas_global\"])\n",
        "\n",
        "    # Salva relatório no Drive\n",
        "    with open(REPORT_PATH, \"w\") as f:\n",
        "        json.dump(stats, f, indent=4)\n",
        "\n",
        "    print(f\"Processamento concluído.\")\n",
        "    print(f\"Vídeo anotado salvo em: {OUTPUT_VIDEO}\")\n",
        "    print(f\"Relatório salvo em: {REPORT_PATH}\")\n",
        "\n",
        "\n",
        "# Execução\n",
        "process_video()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cjm0lj5qW1iU",
        "outputId": "fd8464bf-c2de-4ae7-a686-64bc6b39b0f1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loading models...\n",
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11m-pose.pt to 'yolo11m-pose.pt': 100% ━━━━━━━━━━━━ 40.5MB 37.3MB/s 1.1s\n",
            "Face model loaded successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing V2 Colab:   0%|                                                 | 0/3326 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['lap>=0.5.12'] not found, attempting AutoUpdate...\n",
            "Using Python 3.12.12 environment at: /usr\n",
            "Resolved 2 packages in 122ms\n",
            "Prepared 1 package in 20ms\n",
            "Installed 1 package in 4ms\n",
            " + lap==0.5.12\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 0.6s\n",
            "WARNING ⚠️ \u001b[31m\u001b[1mrequirements:\u001b[0m \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing V2 Colab:   5%|██                                     | 181/3326 [00:21<03:19, 15.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame 181: Nova cena detectada (2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing V2 Colab:  11%|████▎                                  | 364/3326 [00:34<02:45, 17.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame 361: Nova cena detectada (3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing V2 Colab:  16%|██████▍                                | 544/3326 [00:41<01:42, 27.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame 541: Nova cena detectada (4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing V2 Colab:  22%|████████▍                              | 724/3326 [00:49<02:08, 20.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame 721: Nova cena detectada (5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing V2 Colab:  27%|██████████▌                            | 901/3326 [00:58<02:00, 20.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame 901: Nova cena detectada (6)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing V2 Colab:  33%|████████████▎                         | 1082/3326 [01:10<02:31, 14.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame 1081: Nova cena detectada (7)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing V2 Colab:  38%|██████████████▍                       | 1261/3326 [01:21<02:16, 15.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame 1261: Nova cena detectada (8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing V2 Colab:  43%|████████████████▍                     | 1442/3326 [01:40<02:53, 10.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame 1441: Nova cena detectada (9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing V2 Colab:  49%|██████████████████▌                   | 1622/3326 [01:49<01:40, 16.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame 1621: Nova cena detectada (10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing V2 Colab:  52%|███████████████████▌                  | 1713/3326 [01:56<01:42, 15.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame 1711: Nova cena detectada (11)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing V2 Colab:  55%|████████████████████▉                 | 1831/3326 [02:03<01:13, 20.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame 1831: Nova cena detectada (12)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing V2 Colab:  60%|██████████████████████▉               | 2011/3326 [02:12<01:05, 19.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame 2011: Nova cena detectada (13)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing V2 Colab:  71%|███████████████████████████           | 2371/3326 [02:28<00:47, 20.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame 2371: Nova cena detectada (14)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing V2 Colab:  72%|███████████████████████████▍          | 2401/3326 [02:30<00:45, 20.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame 2401: Nova cena detectada (15)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing V2 Colab:  83%|███████████████████████████████▌      | 2762/3326 [02:50<00:34, 16.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame 2761: Nova cena detectada (16)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing V2 Colab:  88%|█████████████████████████████████▌    | 2942/3326 [02:58<00:18, 20.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame 2941: Nova cena detectada (17)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing V2 Colab:  94%|███████████████████████████████████▋  | 3122/3326 [03:06<00:10, 20.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame 3121: Nova cena detectada (18)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing V2 Colab:  99%|█████████████████████████████████████▋| 3301/3326 [03:17<00:01, 14.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame 3301: Nova cena detectada (19)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing V2 Colab: 100%|██████████████████████████████████████| 3326/3326 [03:18<00:00, 16.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processamento concluído.\n",
            "Vídeo anotado salvo em: /content/drive/MyDrive/Colab Notebooks/Fase_4/video_analisado_v2.mp4\n",
            "Relatório salvo em: /content/drive/MyDrive/Colab Notebooks/Fase_4/relatorio_v2.json\n"
          ]
        }
      ]
    }
  ]
}