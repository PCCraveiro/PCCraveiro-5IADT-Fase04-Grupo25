{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMi/dKVnFKRDVRA+axBcYHm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PCCraveiro/PCCraveiro-5IADT-Fase04-Grupo25/blob/main/Tech_Challenge_IADT_Fase_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGr-N3o1NblT",
        "outputId": "3687e651-0fb8-4fe4-f7b6-26a6e07eec38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "✅ Drive montado\n"
          ]
        }
      ],
      "source": [
        "# Monta drive (ggogle drive)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print(\"✅ Drive montado\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalações / Atualizações\n",
        "\n",
        "!pip install -q --upgrade pip\n",
        "\n",
        "# Core do projeto\n",
        "!pip install -q ultralytics fer opencv-python tqdm\n",
        "!pip install -q --upgrade ultralytics\n",
        "!pip install -q fer tqdm\n",
        "\n",
        "print(\"✅ Instalações / Atualizações\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zX51fIpRWoR1",
        "outputId": "45325de0-bf37-49d6-d590-36fc9aa6e143"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.8 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h✅ Instalações / Atualizações\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from ultralytics import YOLO\n",
        "from fer.fer import FER\n",
        "from collections import Counter, deque\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "print(\"✅ Bibliotecas importadas\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2MR0rrHclD4",
        "outputId": "2f2023b1-6d46-4a39-bff0-46863fea5da5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "✅ Bibliotecas importadas\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PARÂMETROS\n",
        "\n",
        "# Pasta base do projeto dentro do seu Drive\n",
        "DRIVE_BASE_DIR = \"/content/drive/MyDrive/Colab Notebooks/Fase_4/\"\n",
        "\n",
        "os.makedirs(DRIVE_BASE_DIR, exist_ok=True)\n",
        "\n",
        "# Paths de entrada/saída no Drive\n",
        "VIDEO_PATH = os.path.join(DRIVE_BASE_DIR, \"video.mp4\")\n",
        "OUTPUT_VIDEO = os.path.join(DRIVE_BASE_DIR, \"video_analisado_v2.mp4\")\n",
        "REPORT_PATH = os.path.join(DRIVE_BASE_DIR, \"relatorio_v2.json\")\n",
        "\n",
        "POSE_MODEL_PATH = \"yolo11m-pose.pt\"     # +13% precisão, ainda rápido\n",
        "\n",
        "CONF_THRESHOLD = 0.35  # mais sensível\n",
        "EMOTION_CONF_THRESHOLD = 0.4  # confiança mínima para aceitar emoção\n",
        "IOU_THRESHOLD = 0.7\n",
        "DEQUE_MAX_LEN = 12\n",
        "\n",
        "# Cores em BGR\n",
        "COLORS = {\n",
        "    \"happy\": (0, 255, 255),\n",
        "    \"sad\": (255, 0, 0),\n",
        "    \"angry\": (0, 0, 255),\n",
        "    \"surprise\": (255, 255, 0),\n",
        "    \"fear\": (255, 0, 255),\n",
        "    \"neutral\": (200, 200, 200),\n",
        "    \"disgust\": (0, 255, 0)\n",
        "}\n",
        "\n",
        "FACE_MODEL_PATH = hf_hub_download(\n",
        "    repo_id=\"arnabdhar/YOLOv8-Face-Detection\",\n",
        "    filename=\"model.pt\",\n",
        "    local_dir=\"/content/models\"\n",
        ")\n",
        "\n",
        "# Baixa tracker otimizado\n",
        "!curl -O https://github.com/ultralytics/ultralytics/raw/main/ultralytics/cfg/trackers/bytetrack.yaml\n",
        "\n",
        "print(f\"✅ Face model baixado: {FACE_MODEL_PATH}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvLoFhT4WrzO",
        "outputId": "32de6452-0e4b-46a0-8088-87f200ad87a3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11m-pose.pt to 'yolo11m-pose.pt': 100% ━━━━━━━━━━━━ 40.5MB 171.7MB/s 0.2s\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "✅ Face model baixado: /content/models/model.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DETECTOR DE CENA\n",
        "class SceneDetector:\n",
        "    \"\"\"\n",
        "    Detector simples de mudança de cena usando histograma de cor em HSV.\n",
        "    Quando a correlação entre histograma atual e anterior fica abaixo\n",
        "    do limiar, considera-se que há uma nova cena.\n",
        "    \"\"\"\n",
        "    def __init__(self, threshold=0.7):\n",
        "        self.threshold = threshold\n",
        "        self.last_hist = None\n",
        "\n",
        "    def detect_scene_change(self, frame):\n",
        "        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
        "        hist = cv2.calcHist([hsv], [0, 1], None, [50, 60], [0, 180, 0, 256])\n",
        "        cv2.normalize(hist, hist, 0, 1, cv2.NORM_MINMAX)\n",
        "\n",
        "        is_new_scene = False\n",
        "        if self.last_hist is not None:\n",
        "            score = cv2.compareHist(self.last_hist, hist, cv2.HISTCMP_CORREL)\n",
        "            if score < self.threshold:\n",
        "                is_new_scene = True\n",
        "\n",
        "        self.last_hist = hist\n",
        "        return is_new_scene"
      ],
      "metadata": {
        "id": "A59T_RlNqZ4H"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CLASSIFICADOR DE AÇÃO\n",
        "class ActionClassifier:\n",
        "    def __init__(self):\n",
        "        self.history = {}\n",
        "        self.velocities = {}\n",
        "\n",
        "    def update(self, track_id, keypoints):\n",
        "        if track_id not in self.history:\n",
        "            self.history[track_id] = deque(maxlen=30)\n",
        "            self.velocities[track_id] = deque(maxlen=10)\n",
        "\n",
        "        self.history[track_id].append(keypoints)\n",
        "\n",
        "        if len(self.history[track_id]) >= 2:\n",
        "            prev_hip = self.history[track_id][-2][11]\n",
        "            curr_hip = keypoints[11]\n",
        "            if prev_hip[0] > 0 and curr_hip[0] > 0:\n",
        "                speed = np.linalg.norm(curr_hip - prev_hip)\n",
        "                self.velocities[track_id].append(speed)\n",
        "\n",
        "    def classify(self, keypoints, track_id):\n",
        "        kpts = keypoints\n",
        "        if kpts.shape[0] < 17:\n",
        "            return \"Desconhecido\"\n",
        "\n",
        "        # 1. QUEDA\n",
        "        shoulder_y = np.mean([kpts[5][1], kpts[6][1]])\n",
        "        hip_y = np.mean([kpts[11][1], kpts[12][1]])\n",
        "        ankle_y = np.mean([kpts[15][1], kpts[16][1]])\n",
        "\n",
        "        if (shoulder_y > 0 and hip_y > 0 and ankle_y > 0 and\n",
        "            shoulder_y > hip_y * 0.9 and hip_y > ankle_y * 0.8):\n",
        "            return \"DEITADO\"\n",
        "\n",
        "        # 2. MÃOS PARA CIMA\n",
        "        nose_y = kpts[0][1]\n",
        "        wrists_y = [kpts[9][1], kpts[10][1]]\n",
        "        if nose_y > 0 and any(w > 0 and w < nose_y * 0.8 for w in wrists_y):\n",
        "            return \"MAOS PARA CIMA\"\n",
        "\n",
        "        # 3. SENTADO (anti-oclução + mesa)\n",
        "        valid_kpts = kpts[kpts[:,0] > 0]\n",
        "        if len(valid_kpts) > 8:\n",
        "            # Critério 1: pernas dobradas OU tronco baixo\n",
        "            knee_hip_dist = (np.linalg.norm(kpts[11] - kpts[13]) +\n",
        "                            np.linalg.norm(kpts[12] - kpts[14]))\n",
        "            shoulder_hip_dist = np.linalg.norm(kpts[5] - kpts[11]) + np.linalg.norm(kpts[6] - kpts[12])\n",
        "\n",
        "            # Critério 2: braços na mesa (cotovelos próximos)\n",
        "            elbow_dist = np.linalg.norm(kpts[7] - kpts[8])\n",
        "\n",
        "            if knee_hip_dist < shoulder_hip_dist * 0.8 or elbow_dist < 80:\n",
        "                return \"SENTADO\"\n",
        "\n",
        "        # 4. CAMINHANDO/CORRENDO (CORRIGIDO)\n",
        "        avg_speed = 0\n",
        "        if track_id in self.velocities and len(self.velocities[track_id]) >= 5:\n",
        "            avg_speed = np.mean(self.velocities[track_id])\n",
        "\n",
        "            if avg_speed > 15:\n",
        "                return \"CORRENDO\"\n",
        "            elif avg_speed > 8:\n",
        "                return \"CAMINHANDO\"\n",
        "\n",
        "        return \"EM PE\""
      ],
      "metadata": {
        "id": "fP1RsX0eqk8X"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DASHBOARD OVERLAY\n",
        "def draw_dashboard(img, frame_emotions, scene_num, scene_people_count, total_unique_people):\n",
        "    \"\"\"\n",
        "    Desenha um pequeno dashboard sobre o frame com:\n",
        "    - Número da cena\n",
        "    - Pessoas na cena e total global\n",
        "    - Barras de emoções detectadas no frame atual\n",
        "    \"\"\"\n",
        "    overlay = img.copy()\n",
        "    h, w, _ = img.shape\n",
        "    dash_w = 280\n",
        "    dash_h = 240\n",
        "    x_start = w - dash_w - 20\n",
        "    y_start = 20\n",
        "\n",
        "    cv2.rectangle(overlay, (x_start, y_start), (x_start + dash_w, y_start + dash_h), (0, 0, 0), -1)\n",
        "    alpha = 0.6\n",
        "    img = cv2.addWeighted(overlay, alpha, img, 1 - alpha, 0)\n",
        "\n",
        "    cv2.putText(img, \"DASHBOARD\", (x_start + 80, y_start + 25),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
        "\n",
        "    y_off = y_start + 55\n",
        "    cv2.putText(img, f\"Cena: {scene_num}\", (x_start + 10, y_off),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
        "    y_off += 25\n",
        "    cv2.putText(img, f\"Pessoas (Cena): {scene_people_count}\", (x_start + 10, y_off),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
        "    y_off += 25\n",
        "    cv2.putText(img, f\"Pessoas (Total): {total_unique_people}\", (x_start + 10, y_off),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (200, 200, 200), 1)\n",
        "\n",
        "    y_off += 30\n",
        "    cv2.putText(img, \"EMOCOES (Ao Vivo)\", (x_start + 10, y_off),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
        "    y_off += 25\n",
        "\n",
        "    total = sum(frame_emotions.values()) if frame_emotions else 1\n",
        "    order = [\"happy\", \"neutral\", \"surprise\", \"fear\", \"angry\", \"sad\"]\n",
        "    for emo in order:\n",
        "        count = frame_emotions.get(emo, 0)\n",
        "        color = COLORS.get(emo, (255, 255, 255))\n",
        "        cv2.putText(img, f\"{emo.capitalize()}\", (x_start + 10, y_off),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
        "        bar_w_max = 120\n",
        "        bar_w = int((count / total) * bar_w_max) if total > 0 else 0\n",
        "        if count > 0 and bar_w < 5:\n",
        "            bar_w = 5\n",
        "        cv2.rectangle(img, (x_start + 90, y_off - 10),\n",
        "                      (x_start + 90 + bar_w, y_off), color, -1)\n",
        "        cv2.putText(img, f\"{count}\", (x_start + 220, y_off),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
        "        y_off += 20\n",
        "\n",
        "    return img"
      ],
      "metadata": {
        "id": "CDlkB2ZfqxUD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ASSOCIAÇÃO ROSTO-PESSOA\n",
        "def get_face_for_person(kpts, face_boxes):\n",
        "    \"\"\"\n",
        "    Encontra a melhor bounding box de rosto para um esqueleto (pessoa).\n",
        "    Usa a média dos keypoints da cabeça (nariz, olhos, orelhas) como centro\n",
        "    e escolhe o rosto mais próximo.\n",
        "    \"\"\"\n",
        "    head_pts = kpts[0:5]\n",
        "    valid_head = head_pts[head_pts[:, 0] > 0]\n",
        "\n",
        "    if len(valid_head) == 0:\n",
        "        return None\n",
        "\n",
        "    head_center = np.mean(valid_head, axis=0)\n",
        "\n",
        "    best_match = None\n",
        "    min_dist = float('inf')\n",
        "\n",
        "    for box in face_boxes:\n",
        "        x1, y1, x2, y2 = box[:4]\n",
        "        face_center = np.array([(x1 + x2) / 2, (y1 + y2) / 2])\n",
        "        dist = np.linalg.norm(head_center - face_center)\n",
        "\n",
        "        if dist < 100:  # limiar em pixels (ajuste conforme resolução)\n",
        "            if dist < min_dist:\n",
        "                min_dist = dist\n",
        "                best_match = box\n",
        "\n",
        "    return best_match"
      ],
      "metadata": {
        "id": "cok_750vq2BC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PROCESSAMENTO DO VÍDEO\n",
        "def process_video():\n",
        "    \"\"\"\n",
        "    Pipeline principal:\n",
        "    - Carrega modelos de pose e rosto (YOLO) em GPU se disponível\n",
        "    - Percorre o vídeo frame a frame\n",
        "    - Detecta pessoas, ações, rostos e emoções\n",
        "    - Detecta mudança de cena\n",
        "    - Gera vídeo anotado e relatório JSON\n",
        "    \"\"\"\n",
        "    # Dispositivo: prioriza CUDA (A100), caso não exista cai para CPU\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    print(\"Loading models...\")\n",
        "    # Ultralytics carrega automaticamente o modelo local ou baixa da internet\n",
        "    pose_model = YOLO(POSE_MODEL_PATH)  # modelo de pose\n",
        "    try:\n",
        "        face_model = YOLO(FACE_MODEL_PATH)  # modelo de face\n",
        "        print(\"Face model loaded successfully.\")\n",
        "    except Exception as e:\n",
        "        face_model = None\n",
        "        print(f\"WARNING: Face model failed to load: {e}\")\n",
        "\n",
        "    # Detector de emoções FER (usa internamente CNN + OpenCV)\n",
        "    detector = FER(mtcnn=False)\n",
        "\n",
        "    # Abre o vídeo (no Drive)\n",
        "    cap = cv2.VideoCapture(VIDEO_PATH)\n",
        "    if not cap.isOpened():\n",
        "        raise FileNotFoundError(f\"Não foi possível abrir o vídeo em: {VIDEO_PATH}\")\n",
        "\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS) or 25.0\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    # Writer para o vídeo anotado (no Drive)\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    writer = cv2.VideoWriter(OUTPUT_VIDEO, fourcc, fps, (width, height))\n",
        "\n",
        "    action_classifier = ActionClassifier()\n",
        "    scene_detector = SceneDetector(threshold=0.7)\n",
        "\n",
        "    # Estatísticas finais (para o JSON)\n",
        "    stats = {\n",
        "        \"emocoes\": Counter(),\n",
        "        \"acoes\": Counter(),\n",
        "        \"frames\": 0,\n",
        "        \"total_cenas\": 0,\n",
        "        \"pessoas_por_cena\": {},  # {scene_id: count}\n",
        "        \"pessoas_global\": set()  # será convertido para lista na hora do dump\n",
        "    }\n",
        "\n",
        "    emotion_buffer = {}\n",
        "    last_emotions = {}\n",
        "\n",
        "    # Controle de cenas e IDs locais\n",
        "    scene_counter = 1\n",
        "    scene_map = {}  # {global_track_id: local_scene_id}\n",
        "    next_scene_person_id = 1\n",
        "    current_scene_people = set()\n",
        "\n",
        "    pbar = tqdm(total=total_frames, desc=\"Processing V2 Colab\", ncols=100)\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        stats[\"frames\"] += 1\n",
        "        annotated_frame = frame.copy()\n",
        "        current_frame_emotions = Counter()\n",
        "\n",
        "        # 1) Detecção de mudança de cena\n",
        "        if scene_detector.detect_scene_change(frame):\n",
        "            scene_counter += 1\n",
        "            scene_map = {}\n",
        "            next_scene_person_id = 1\n",
        "            current_scene_people = set()\n",
        "            action_classifier.history = {}\n",
        "            print(f\"Frame {stats['frames']}: Nova cena detectada ({scene_counter})\")\n",
        "\n",
        "        # 2) Detecção de rostos (opcional)\n",
        "        face_boxes = []\n",
        "        if face_model:\n",
        "            face_results = face_model(frame, verbose=False, conf=0.4, device=device, half=True)\n",
        "            if face_results and face_results[0].boxes is not None:\n",
        "                face_boxes = face_results[0].boxes.xyxy.detach().cpu().numpy()\n",
        "\n",
        "        # 3) Pose + tracking\n",
        "        pose_results = pose_model.track(\n",
        "            frame,\n",
        "            persist=True,\n",
        "            verbose=False,\n",
        "            device=device,\n",
        "            conf=CONF_THRESHOLD,\n",
        "            iou=IOU_THRESHOLD,\n",
        "            classes=[0], # só pessoas\n",
        "            half=True  # FP16\n",
        "        )\n",
        "\n",
        "        # FILTRA POSES RUINS (robôs, objetos)\n",
        "        if pose_results and pose_results[0].keypoints is not None:\n",
        "\n",
        "            kpts_all = pose_results[0].keypoints.xy.detach().cpu().numpy()\n",
        "\n",
        "            if pose_results[0].keypoints.conf is not None:\n",
        "                kpts_conf = pose_results[0].keypoints.conf.detach().cpu().numpy()\n",
        "            else:\n",
        "                kpts_conf = np.ones((kpts_all.shape[0], kpts_all.shape[1]))\n",
        "\n",
        "            ids = pose_results[0].boxes.id\n",
        "            if ids is not None:\n",
        "                ids = ids.int().detach().cpu().tolist()\n",
        "            else:\n",
        "                ids = [-1] * len(kpts_all)\n",
        "\n",
        "            valid_indices = []\n",
        "\n",
        "            for i, (kpts, confs) in enumerate(zip(kpts_all, kpts_conf)):\n",
        "                num_good_kpts = np.sum(\n",
        "                    (kpts[:, 0] > 0) &\n",
        "                    (kpts[:, 1] > 0) &\n",
        "                    (confs > 0.3)\n",
        "                )\n",
        "                if num_good_kpts >= 10:\n",
        "                    valid_indices.append(i)\n",
        "\n",
        "            # aplica filtro UMA ÚNICA VEZ\n",
        "            if valid_indices:\n",
        "                kpts_all = kpts_all[valid_indices]\n",
        "                ids = [ids[i] if i < len(ids) else -1 for i in valid_indices]\n",
        "            else:\n",
        "                kpts_all = []\n",
        "                ids = []\n",
        "\n",
        "        if pose_results and pose_results[0].keypoints is not None:\n",
        "            kpts_all = pose_results[0].keypoints.xy.detach().cpu().numpy()\n",
        "            ids = pose_results[0].boxes.id\n",
        "            if ids is not None:\n",
        "                ids = ids.int().detach().cpu().tolist()\n",
        "            else:\n",
        "                ids = [-1] * len(kpts_all)\n",
        "\n",
        "            for kpts, track_id in zip(kpts_all, ids):\n",
        "                # Mapeamento de ID global -> ID local na cena\n",
        "                if track_id != -1:\n",
        "                    stats[\"pessoas_global\"].add(int(track_id))\n",
        "                    if track_id not in scene_map:\n",
        "                        scene_map[track_id] = next_scene_person_id\n",
        "                        next_scene_person_id += 1\n",
        "\n",
        "                    local_id = scene_map[track_id]\n",
        "                    current_scene_people.add(local_id)\n",
        "                else:\n",
        "                    local_id = \"?\"\n",
        "\n",
        "                # Atualiza classificador de ação\n",
        "                action_classifier.update(track_id, kpts)\n",
        "                action = action_classifier.classify(kpts, track_id)\n",
        "                stats[\"acoes\"][action] += 1\n",
        "\n",
        "                # Desenha esqueleto\n",
        "                skeleton = [\n",
        "                    (5, 7), (7, 9), (6, 8), (8, 10),\n",
        "                    (5, 6), (5, 11), (6, 12),\n",
        "                    (11, 12), (11, 13), (13, 15),\n",
        "                    (12, 14), (14, 16)\n",
        "                ]\n",
        "                for p1, p2 in skeleton:\n",
        "                    if kpts[p1][0] > 0 and kpts[p2][0] > 0:\n",
        "                        pt1 = (int(kpts[p1][0]), int(kpts[p1][1]))\n",
        "                        pt2 = (int(kpts[p2][0]), int(kpts[p2][1]))\n",
        "                        cv2.line(annotated_frame, pt1, pt2, (0, 255, 0), 2)\n",
        "\n",
        "                # Emoções\n",
        "                emotion_label = last_emotions.get(track_id, \"neutral\")\n",
        "\n",
        "                # Tenta associar um rosto detectado a essa pessoa\n",
        "                matched_face = get_face_for_person(kpts, face_boxes)\n",
        "                face_img = None\n",
        "\n",
        "                if matched_face is not None:\n",
        "                    x1, y1, x2, y2 = matched_face[:4].astype(int)\n",
        "                    pad = 10\n",
        "                    x1 = max(0, x1 - pad)\n",
        "                    y1 = max(0, y1 - pad)\n",
        "                    x2 = min(width, x2 + pad)\n",
        "                    y2 = min(height, y2 + pad)\n",
        "                    if x2 > x1 and y2 > y1:\n",
        "                        face_img = frame[y1:y2, x1:x2]\n",
        "\n",
        "                # fallback: recorta região da cabeça a partir dos keypoints\n",
        "                elif np.any(kpts[0:5, 0] > 0):\n",
        "                    face_pts = kpts[0:5]\n",
        "                    valid_face_pts = face_pts[face_pts[:, 0] > 0]\n",
        "                    if len(valid_face_pts) > 0:\n",
        "                        fx1, fy1 = np.min(valid_face_pts, axis=0).astype(int)\n",
        "                        fx2, fy2 = np.max(valid_face_pts, axis=0).astype(int)\n",
        "                        pad = 30\n",
        "                        fx1 = max(0, fx1 - pad)\n",
        "                        fy1 = max(0, fy1 - pad)\n",
        "                        fx2 = min(width, fx2 + pad)\n",
        "                        fy2 = min(height, fy2 + pad)\n",
        "                        if fx2 > fx1 and fy2 > fy1:\n",
        "                            face_img = frame[fy1:fy2, fx1:fx2]\n",
        "\n",
        "                # Inferência de emoção se tivermos uma face recortada válida\n",
        "                if face_img is not None and face_img.size > 0 and face_img.shape[0] > 20 and face_img.shape[1] > 20:\n",
        "                    try:\n",
        "                        emotions = detector.detect_emotions(face_img)\n",
        "                        if emotions:\n",
        "                            top_emotion, score = max(\n",
        "                                emotions[0][\"emotions\"].items(),\n",
        "                                key=lambda x: x[1]\n",
        "                            )\n",
        "                            if score > EMOTION_CONF_THRESHOLD:\n",
        "                                if track_id not in emotion_buffer:\n",
        "                                    emotion_buffer[track_id] = deque(maxlen=DEQUE_MAX_LEN)\n",
        "                                emotion_buffer[track_id].append(top_emotion)\n",
        "                                most_common = Counter(emotion_buffer[track_id]).most_common(1)[0][0]\n",
        "                                emotion_label = most_common\n",
        "                                last_emotions[track_id] = emotion_label\n",
        "                    except Exception:\n",
        "                        pass\n",
        "\n",
        "                stats[\"emocoes\"][emotion_label] += 1\n",
        "                current_frame_emotions[emotion_label] += 1\n",
        "\n",
        "                # Desenha label da pessoa\n",
        "                if np.any(kpts[:, 0] > 0):\n",
        "                    valid_x = kpts[kpts[:, 0] > 0, 0]\n",
        "                    valid_y = kpts[kpts[:, 0] > 0, 1]\n",
        "                    x1, y1 = int(np.min(valid_x)), int(np.min(valid_y))\n",
        "\n",
        "                    color = COLORS.get(emotion_label, (0, 255, 0))\n",
        "\n",
        "                    # Calcula proximidade com outras pessoas\n",
        "                    # Label melhorado com contexto grupo\n",
        "                    nearby_count = len(current_scene_people) - 1  # total na cena menos eu\n",
        "                    label_text = f\"Pessoa {local_id} | {action}\"\n",
        "                    if nearby_count > 1:\n",
        "                        label_text += f\" | Grupo:{nearby_count}\"\n",
        "\n",
        "                    emo_label = f\"{emotion_label.upper()}\"\n",
        "\n",
        "                    (tw, th), _ = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n",
        "                    cv2.rectangle(annotated_frame, (x1, y1-50), (x1 + max(220, tw), y1), color, -1)\n",
        "                    cv2.putText(annotated_frame, label_text, (x1, y1-25), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2)\n",
        "                    cv2.putText(annotated_frame, emo_label, (x1, y1-5), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
        "\n",
        "\n",
        "        # Atualiza estatísticas por cena\n",
        "        stats[\"total_cenas\"] = scene_counter\n",
        "        stats[\"pessoas_por_cena\"][f\"Cena {scene_counter}\"] = len(current_scene_people)\n",
        "\n",
        "        annotated_frame = draw_dashboard(\n",
        "            annotated_frame,\n",
        "            current_frame_emotions,\n",
        "            scene_counter,\n",
        "            len(current_scene_people),\n",
        "            len(stats[\"pessoas_global\"])\n",
        "        )\n",
        "\n",
        "        writer.write(annotated_frame)\n",
        "        pbar.update(1)\n",
        "\n",
        "    cap.release()\n",
        "    writer.release()\n",
        "    pbar.close()\n",
        "\n",
        "    # Converte set para lista para serializar em JSON\n",
        "    stats[\"pessoas_global\"] = list(stats[\"pessoas_global\"])\n",
        "\n",
        "    # Salva relatório no Drive\n",
        "    with open(REPORT_PATH, \"w\") as f:\n",
        "        json.dump(stats, f, indent=4)\n",
        "\n",
        "    print(f\"Processamento concluído.\")\n",
        "    print(f\"Vídeo anotado salvo em: {OUTPUT_VIDEO}\")\n",
        "    print(f\"Relatório salvo em: {REPORT_PATH}\")\n",
        "\n",
        "\n",
        "# Execução\n",
        "process_video()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cjm0lj5qW1iU",
        "outputId": "ede7165e-5e96-4ea5-e6de-725cbe5c90d2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loading models...\n",
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11m-pose.pt to 'yolo11m-pose.pt': 100% ━━━━━━━━━━━━ 40.5MB 168.3MB/s 0.2s\n",
            "Face model loaded successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing V2 Colab:   0%|                                                 | 0/3326 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['lap>=0.5.12'] not found, attempting AutoUpdate...\n",
            "Using Python 3.12.12 environment at: /usr\n",
            "Resolved 2 packages in 101ms\n",
            "Prepared 1 package in 103ms\n",
            "Installed 1 package in 4ms\n",
            " + lap==0.5.12\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 0.6s\n",
            "WARNING ⚠️ \u001b[31m\u001b[1mrequirements:\u001b[0m \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing V2 Colab:   5%|██                                     | 181/3326 [00:20<03:16, 15.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame 181: Nova cena detectada (2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing V2 Colab:  11%|████▎                                  | 364/3326 [00:33<02:45, 17.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame 361: Nova cena detectada (3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing V2 Colab:  16%|██████▎                                | 542/3326 [00:39<01:39, 28.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame 541: Nova cena detectada (4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing V2 Colab:  22%|████████▍                              | 724/3326 [00:48<02:02, 21.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame 721: Nova cena detectada (5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing V2 Colab:  27%|██████████▌                            | 901/3326 [00:56<01:58, 20.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame 901: Nova cena detectada (6)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing V2 Colab:  33%|████████████▎                         | 1082/3326 [01:09<02:30, 14.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame 1081: Nova cena detectada (7)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing V2 Colab:  38%|██████████████▍                       | 1261/3326 [01:19<02:16, 15.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame 1261: Nova cena detectada (8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing V2 Colab:  43%|████████████████▍                     | 1442/3326 [01:38<02:49, 11.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame 1441: Nova cena detectada (9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing V2 Colab:  49%|██████████████████▌                   | 1621/3326 [01:47<01:33, 18.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame 1621: Nova cena detectada (10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing V2 Colab:  52%|███████████████████▌                  | 1714/3326 [01:53<01:39, 16.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame 1711: Nova cena detectada (11)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing V2 Colab:  55%|████████████████████▉                 | 1833/3326 [02:00<01:11, 20.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame 1831: Nova cena detectada (12)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing V2 Colab:  60%|██████████████████████▉               | 2011/3326 [02:09<01:05, 20.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame 2011: Nova cena detectada (13)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing V2 Colab:  71%|███████████████████████████           | 2374/3326 [02:26<00:45, 20.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame 2371: Nova cena detectada (14)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing V2 Colab:  72%|███████████████████████████▍          | 2401/3326 [02:27<00:44, 20.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame 2401: Nova cena detectada (15)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing V2 Colab:  83%|███████████████████████████████▌      | 2762/3326 [02:47<00:32, 17.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame 2761: Nova cena detectada (16)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing V2 Colab:  88%|█████████████████████████████████▌    | 2942/3326 [02:55<00:18, 20.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame 2941: Nova cena detectada (17)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing V2 Colab:  94%|███████████████████████████████████▋  | 3122/3326 [03:03<00:10, 20.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame 3121: Nova cena detectada (18)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing V2 Colab:  99%|█████████████████████████████████████▋| 3301/3326 [03:13<00:01, 15.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame 3301: Nova cena detectada (19)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing V2 Colab: 100%|██████████████████████████████████████| 3326/3326 [03:15<00:00, 17.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processamento concluído.\n",
            "Vídeo anotado salvo em: /content/drive/MyDrive/Colab Notebooks/Fase_4/video_analisado_v2.mp4\n",
            "Relatório salvo em: /content/drive/MyDrive/Colab Notebooks/Fase_4/relatorio_v2.json\n"
          ]
        }
      ]
    }
  ]
}